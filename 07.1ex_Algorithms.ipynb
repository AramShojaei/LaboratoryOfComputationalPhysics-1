{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **PCA on 3D dataset**\n",
    "\n",
    "* Generate a dataset with 3 features each with N entries (N being ${\\cal O}(1000)$). With $N(\\mu,\\sigma)$ the normali distribution with mean $\\mu$ and $\\sigma$  standard deviation, generate the 3 variables $x_{1,2,3}$ such that:\n",
    "    * $x_1$ is distributed as $N(0,1)$\n",
    "    * $x_2$ is distributed as $x_1+N(0,3)$\n",
    "    * $x_3$ is given by $2x_1+x_2$\n",
    "* Find the eigenvectors and eigenvalues of the covariance matrix of the dataset\n",
    "* Find the eigenvectors and eigenvalues using SVD. Check that they are two procedure yields to same result\n",
    "* What percent of the total variability is explained by the principal components? Given how the dataset was constructed, do these make sense? Reduce the dimensionality of the system so that at least 99% of the total variability is retained.\n",
    "* Redefine the data in the basis yielded by the PCA procedure\n",
    "* Plot the data points in the original and the new coordiantes as a set of scatter plots. Your final figure should have 2 rows of 3 plots each, where the columns show the (0,1), (0,2) and (1,2) proejctions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.linalg as la\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x1 = np.random.normal(0,1,size=N)\n",
    "x2 = x1 + np.random.normal(0,3,size=N)\n",
    "x3 = 2*x1 + x2\n",
    "X = np.array([x1,x2,x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(X)\n",
    "l, V = np.linalg.eig(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, spectrum, Vt = np.linalg.svd(X)\n",
    "l_svd = spectrum**2/(N-1)\n",
    "V_svd = U\n",
    "\n",
    "print(\"Shape {}\".format(X.shape))\n",
    "print (\"Numpy eig:\\n\")\n",
    "print (\"eigenvalues:\",l)\n",
    "print (\"eigenvectors:\",V)\n",
    "print (\"\\nSVD\\n\")\n",
    "print (\"eigenvalues:\",l_svd)\n",
    "print (\"eigenvectors:\",V_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTICE\n",
    "\n",
    "The results are correct, just swap the first and the third column. Difference due to machine precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using svd ordered results\n",
    "Lambda=np.diag(l_svd)\n",
    "print (\"Lambda {}\".format(Lambda))\n",
    "#cov matrix Trace\n",
    "print (\"Covariance trace:\", cov.trace())\n",
    "#lambda trace\n",
    "print (\"Lambda trace:\", Lambda.trace())\n",
    "\n",
    "print (\"Percentage: {}\".format(Lambda[0,0]/Lambda.trace()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda=np.diag(l_svd[0:2])\n",
    "print (\"Lambda {}\".format(Lambda))\n",
    "#cov matrix Trace\n",
    "print (\"Covariance trace:\", cov.trace())\n",
    "#lambda trace\n",
    "print (\"Lambda trace:\", Lambda.trace())\n",
    "\n",
    "print (\"Percentage: {}\".format(Lambda[0,0]/Lambda.trace()))\n",
    "print (\"Trace not vary. Right result!\")\n",
    "print (\"Total variability: {}\".format((l_svd[0]+l_svd[1])/sum(l_svd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = np.dot(X.T, U).T\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=3, figsize=(15,10))\n",
    "ax1[0].scatter(X[0,:],X[1,:], alpha=0.2)\n",
    "ax1[1].scatter(X[0,:],X[2,:], alpha=0.2)\n",
    "ax1[2].scatter(X[1,:],X[2,:], alpha=0.2)\n",
    "\n",
    "ax2[0].scatter(Xp[0,:],Xp[1,:], alpha=0.2)\n",
    "ax2[1].scatter(Xp[0,:],Xp[2,:], alpha=0.2)\n",
    "ax2[2].scatter(Xp[1,:],Xp[2,:], alpha=0.2)\n",
    "\n",
    "for j in range(0,3):\n",
    "    #not same axis limit to get some comparison with seaborn\n",
    "    ax1[j].axis([-10,10,-15,15])\n",
    "    ax2[j].axis([-15,15,-5,5])\n",
    "    \n",
    "ax1[0].set_title(\"old {} vs {}\".format(1,2))\n",
    "ax2[0].set_title(\"new {} vs {}\".format(1,2))\n",
    "ax1[1].set_title(\"old {} vs {}\".format(1,3))\n",
    "ax2[1].set_title(\"new {} vs {}\".format(1,3))\n",
    "ax1[2].set_title(\"old {} vs {}\".format(2,3))\n",
    "ax2[2].set_title(\"new {} vs {}\".format(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x1':x1, 'x2':x2,'x3':x3})\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(Xp.T)\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. **PCA on a nD dataset**\n",
    "\n",
    "Start from the dataset you have genereted in the previous exercise and add uncorrelated random noise. Such noise should be represented by other 10 uncorrelated variables normal distributed, with standar deviation much smaller (say, a factor 50) than those used to generate the $x_1$ and $x_2$.\n",
    "\n",
    "Repeat the PCA procedure and compare the results with what you obtained before\n",
    "\n",
    "\n",
    "We retain less variability. The correlation between 1st and 3rd components and between 2nd and 3rd is less with respect to the case before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.linalg as la\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "n1 = np.zeros((10,N))\n",
    "n2 = np.zeros((10,N))\n",
    "n3 = np.zeros((10,N))\n",
    "for i in range(10):\n",
    "    mean = 0\n",
    "    std = np.random.uniform(0.01,0.03)\n",
    "    n1[i] = np.random.normal(mean,std,N)\n",
    "    std = np.random.uniform(0.01,0.12)\n",
    "    n2[i] = np.random.normal(mean,std,N)\n",
    "    std = np.random.uniform(0.01,0.32)\n",
    "    n2[i] = np.random.normal(mean,std,N)\n",
    "\n",
    "x1 = np.random.normal(0,1,1000) + n1.sum(axis=0)\n",
    "x2 = np.random.normal(0,3,1000) + x1 + n2.sum(axis=0)\n",
    "df = pd.DataFrame({'x1':x1,'x2':x2,'x3':(2*x1+x2+n2.sum(axis=0)+n1.sum(axis=0))})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(df, rowvar=False)\n",
    "l, V = np.linalg.eig(cov)\n",
    "print (\"Covariance:{}\\n\".format(cov))\n",
    "\n",
    "U, spectrum, Vt = np.linalg.svd(df.T)\n",
    "l_svd = spectrum**2/(N-1)\n",
    "V_svd = U\n",
    "\n",
    "print (\"Numpy eig:\\n\")\n",
    "print (\"eigenvalues:\",l)\n",
    "print (\"eigenvectors:\",V)\n",
    "print (\"\\nSVD\\n\")\n",
    "print (\"eigenvalues:\",l_svd)\n",
    "print (\"eigenvectors:\",V_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_variability = l/np.sum(l)\n",
    "\n",
    "print('variability per principal component:',found_variability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using svd ordered results\n",
    "Lambda=np.diag(l_svd)\n",
    "print (\"Lambda {}\".format(Lambda))\n",
    "#cov matrix Trace\n",
    "print (\"Covariance trace:\", cov.trace())\n",
    "#lambda trace\n",
    "print (\"Lambda trace:\", Lambda.trace())\n",
    "\n",
    "print (\"Percentage: {}\".format(Lambda[0,0]/Lambda.trace()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using svd ordered results\n",
    "Lambda=np.diag(l_svd[0:2])\n",
    "print (\"Lambda {}\".format(Lambda))\n",
    "#cov matrix Trace\n",
    "print (\"Covariance trace:\", cov.trace())\n",
    "#lambda trace\n",
    "print (\"Lambda trace:\", Lambda.trace())\n",
    "\n",
    "print (\"Percentage: {}\".format(Lambda[0,0]/Lambda.trace()))\n",
    "\n",
    "print (\"Trace not vary. Right result!\")\n",
    "print (\"Total variability: {}\".format((l_svd[0]+l_svd[1])/sum(l_svd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = np.dot(df, U).T\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=3, figsize=(15,10))\n",
    "ax1[0].scatter(df.iloc[:,0],df.iloc[:,1], alpha=0.2)\n",
    "ax1[1].scatter(df.iloc[:,0],df.iloc[:,2], alpha=0.2)\n",
    "ax1[2].scatter(df.iloc[:,1],df.iloc[:,2], alpha=0.2)\n",
    "\n",
    "ax2[0].scatter(Xp[0,:],Xp[1,:], alpha=0.2)\n",
    "ax2[1].scatter(Xp[0,:],Xp[2,:], alpha=0.2)\n",
    "ax2[2].scatter(Xp[1,:],Xp[2,:], alpha=0.2)\n",
    "\n",
    "for j in range(0,3):\n",
    "    #not same axis limit to get some comparison with seaborn\n",
    "    ax1[j].axis([-10,10,-15,15])\n",
    "    ax2[j].axis([-15,15,-5,5])\n",
    "    \n",
    "ax1[0].set_title(\"old {} vs {}\".format(1,2))\n",
    "ax2[0].set_title(\"new {} vs {}\".format(1,2))\n",
    "ax1[1].set_title(\"old {} vs {}\".format(1,3))\n",
    "ax2[1].set_title(\"new {} vs {}\".format(1,3))\n",
    "ax1[2].set_title(\"old {} vs {}\".format(2,3))\n",
    "ax2[2].set_title(\"new {} vs {}\".format(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "sns.pairplot(pd.DataFrame(Xp.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 \\. **Looking at an oscillating spring** (optional)\n",
    "\n",
    "Imagine you have $n$ cameras looking at a spring oscillating along the $x$ axis. Each  camera record the motion of the spring looking at it along a given direction defined by the pair $(\\theta_i, \\phi_i)$, the angles in spherical coordinates. \n",
    "\n",
    "Start from the simulation of the records (say ${\\cal O}(1000)$) of the spring's motion along the x axis, assuming a little random noise affects the measurements along the $y$. Rotate such dataset to emulate the records of each camera.\n",
    "\n",
    "Perform a Principal Component Analysis on the thus obtained dataset, aiming at finding the only one coordinate that really matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 2\n",
    "A = 1\n",
    "N = 1000\n",
    "data = np.zeros((10,N,3))\n",
    "phi = 0\n",
    "time = np.arange(0, 10, 10/N)\n",
    "\n",
    "x = A*np.sin(w*time+phi)\n",
    "y = np.random.normal(0, 1/5, N)\n",
    "z = np.zeros(N)\n",
    "data[0] = np.array([x,y,z]).T\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time,x,'*')\n",
    "plt.title(\"X-motion in time\")\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('$x$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time,y,'*')\n",
    "plt.title(\"Y-motion in time(noise)\")\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('$y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "i = 1\n",
    "for theta in [math.pi/6,math.pi/4,math.pi/2]:\n",
    "    for gamma in [math.pi/6,math.pi/4,math.pi/3]:\n",
    "        R1 = [[math.cos(theta), math.sin(theta), 0], [-math.sin(theta), math.cos(theta), 0], [0,0,1]]\n",
    "        R2 = [[math.cos(gamma), 0, -math.sin(gamma)], [0,1,0] , [math.sin(gamma), 0, math.cos(gamma)]]\n",
    "        R = np.matmul(R1,R2)\n",
    "        data_rot = np.matmul(R,data[0].T)\n",
    "        data[i] = data_rot.T\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, plots = plt.subplots(nrows=5, ncols=2, figsize=(10,10))\n",
    "fig.suptitle(\"X-motion wrt time and camera changes\", fontsize=10)\n",
    "i = 0\n",
    "j = 0\n",
    "for record in data:\n",
    "    plots[j][i].plot(time, record[:,0],'*')\n",
    "    plots[j][i].set_aspect('equal')\n",
    "    i+=1\n",
    "    if i == 2:\n",
    "        j += 1\n",
    "        i = 0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, plots = plt.subplots(nrows=5, ncols=2, figsize=(10,10))\n",
    "fig.suptitle(\"Y-motion wrt time and camera changes\", fontsize=10)\n",
    "i = 0\n",
    "j = 0\n",
    "for record in data:\n",
    "    plots[j][i].plot(time, record[:,1],'*')\n",
    "    plots[j][i].set_aspect('equal')\n",
    "    i+=1\n",
    "    if i == 2:\n",
    "        j += 1\n",
    "        i = 0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(data[:,:,0])\n",
    "l, v = np.linalg.eig(cov)\n",
    "\n",
    "U, s, Vt = np.linalg.svd(cov)\n",
    "\n",
    "found_variability = l/np.sum(l)\n",
    "\n",
    "print('variability per principal component for x direction:',found_variability)\n",
    "summed = 0\n",
    "indexes = []\n",
    "var = found_variability.copy()\n",
    "for i in range(0, found_variability.size):\n",
    "    summed += found_variability[var.argmax()]\n",
    "    indexes.append(var.argmax())\n",
    "    var[var.argmax()] = 0\n",
    "    if summed > 0.92:\n",
    "        break\n",
    "\n",
    "\n",
    "print('cameras {} retain ~{} of the total variability'.format(indexes, found_variability[indexes].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. **PCA on the MAGIC dataset** (optional)\n",
    "\n",
    "Perform a PCA on the magic04.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/magic04.data', names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'fClass'])\n",
    "df.loc[df['fClass'] =='h', 'fClass'] = 0\n",
    "df.loc[df['fClass'] =='g', 'fClass'] = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(df, rowvar=False)\n",
    "l, V = np.linalg.eig(cov)\n",
    "#print (\"Covariance:{}\\n\".format(cov))\n",
    "\n",
    "U, spectrum, Vt = np.linalg.svd(df.T)\n",
    "l_svd = spectrum**2/(df.shape[0]-1)\n",
    "V_svd = U\n",
    "\n",
    "print (\"Numpy eig:\\n\")\n",
    "print (\"eigenvalues:\",l)\n",
    "print (\"eigenvectors:\",V)\n",
    "print (\"\\nSVD\\n\")\n",
    "print (\"eigenvalues:\",l_svd)\n",
    "print (\"eigenvectors:\",V_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_variability = l/np.sum(l)\n",
    "\n",
    "print('variability per principal component:',found_variability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using svd ordered results\n",
    "Lambda=np.diag(l_svd)\n",
    "print (\"Lambda {}\".format(Lambda))\n",
    "#cov matrix Trace\n",
    "print (\"Covariance trace:\", cov.trace())\n",
    "#lambda trace\n",
    "print (\"Lambda trace:\", Lambda.trace())\n",
    "\n",
    "print (\"Percentage: {}\".format(Lambda[0,0]/Lambda.trace()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed = 0\n",
    "indexes = []\n",
    "var = found_variability.copy()\n",
    "for i in range(0, found_variability.size):\n",
    "    summed += found_variability[var.argmax()]\n",
    "    indexes.append(var.argmax())\n",
    "    var[var.argmax()] = 0\n",
    "    if summed > 0.9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using svd ordered results\n",
    "Lambda=np.diag(l_svd[indexes])\n",
    "print (\"Lambda {}\".format(Lambda))\n",
    "#cov matrix Trace\n",
    "print (\"Covariance trace:\", cov.trace())\n",
    "#lambda trace\n",
    "print (\"Lambda trace:\", Lambda.trace())\n",
    "\n",
    "print (\"Percentage: {}\".format(Lambda[0,0]/Lambda.trace()))\n",
    "\n",
    "print (\"Trace not vary. Right result!\")\n",
    "print (\"Total variability: {}\".format((l_svd[0]+l_svd[1]+l_svd[2]+l_svd[3])/sum(l_svd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = np.dot(df, V).T\n",
    "sns.pairplot(df)\n",
    "sns.pairplot(pd.DataFrame(Xp.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
